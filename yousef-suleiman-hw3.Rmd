---
title: "CS 422 HW 3"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
author: Yousef Suleiman
---

### Set-Up
```{r}
library(ISLR)
setwd("G:/My Drive/Sophomore Fall/CS 422 Data Mining/R primer")

set.seed(1122)
index <- sample(1:nrow(Auto), 0.95*dim(Auto)[1])
train.df <- Auto[index,]
test.df <- Auto[-index,]
```

### Part 2-A
```{r}
model <- lm(mpg ~ ., data = train.df[,-9])
```
#### Part 2-A-i
It is not a reasonable thing to do because name is more of an abstract identifier so it has no value as a predictor.

#### Part 2-A-ii
```{r}
summary(model)
R2 <- summary(model)$r.squared
RSE <- summary(model)$sigma
RMSE <- sqrt( mean(model$residuals^2) )
paste0("R^2 is ", R2, ".")
paste0("The residual standard error is ", RSE, ".")
paste0("The root mean square error is ", RMSE, ".")
```
The model has a good fit as R^2 is close to 1 and it is able to explain around 81.7% of the variation. The RSE is also not too high and RMSE tells us that the average distance between the predicted value of mpg and the actual is 3.331 mpg which is acceptable. 

#### Part 2-A-iii
```{r}
plot(model, 1)
```

#### Part 2-A-iv
```{r}
hist(model$residuals,
     main = "Histogram of the Residuals from model",
     xlab = "Residual")
```
Because the histogram follows a Gaussian distribution, we can see that as residuals come closer to 0, they become more frequent. Compared to data points with greater error, the ones with less are more frequent.

### Part 2-B
#### Part 2-B-i
```{r}
model_2 <- lm(mpg ~ ., data = train.df[,-9])
```
#### Part 2-B-ii
```{r}
model_2 <- lm(mpg ~ ., data = train.df[,c(-2,-3,-4,-6,-9)])
summary(model_2)
R2_2 <- summary(model_2)$r.squared
RSE_2 <- summary(model_2)$sigma
RMSE_2 <- sqrt( mean(model_2$residuals^2) )
paste0("R^2 is ", R2_2, ".")
paste0("The residual standard error is ", RSE_2, ".")
paste0("The root mean square error is ", RMSE_2, ".")
```
The model still has a good fit although it is slightly less. R^1 isn't as close to 1 but it describes 81.2% of the distribution (not too different from the previous 81.7%). The RSE and RMSE also have slightly increased from 3.367 to 3.389 and 3.331 to 3.371 which is not horrible. RMSE nows reveals that the average distance between predicted and actual values is 3.371 mpg.

#### Part 2-B-iii
```{r}
plot(model_2, 1)
```

#### Part 2-B-iv
```{r}
hist(model_2$residuals,
     main = "Histogram of the Residuals from model_2",
     xlab = "Residual")
```
Again, the histogram follows a Gaussian distribution so we can see that as residuals come closer to between 0 and 1, they become more frequent. Compared to data points with greater error, the ones with less are more frequent.

#### Part 2-B-v
Even though the R^2, RSE, and RMSE all moved slightly in directions opposite to what is desired, model_2 is still simpler. The expense of a minute amount of accuracy (considering that we are only predicting mpg) is worth it because we are using more than 50% less predictors. This is both more cost-effective for whatever enterprise would be using this model and better in terms of Occam's Razor.

### Part 2-C
```{r}
mpg.df <- data.frame(Prediction = predict(model_2, newdata = test.df),
                    Responce = test.df[,1])
mpg.df
```
### Part 2-D
```{r}
pred <- predict(model_2, newdata = test.df, interval = 'confidence', level = 0.95)
mpg.df <- data.frame( Prediction = pred[,1], Response = test.df[,1], Lower = pred[,2], Upper = pred[,3], 
                     Matches = as.integer(test.df[,1] >= pred[,2] & test.df[,1] <= pred[,3]) )
mpg.df
paste0("Total observations correctly predicted:", sum(mpg.df$Matches))
```

### Part 2-E
```{r}
pred <- predict(model_2, newdata = test.df, interval = 'prediction', level = 0.95)
mpg.df <- data.frame( Prediction = pred[,1], Response = test.df[,1], Lower = pred[,2], Upper = pred[,3], 
                     Matches = as.integer(test.df[,1] >= pred[,2] & test.df[,1] <= pred[,3]) )
mpg.df
paste0("Total observations correctly predicted:", sum(mpg.df$Matches))
```

### Part 2-F
#### Part 2-F-i
Part E, the prediction interval, results in much more matches.

#### Part 2-F-ii
This is because the prediction interval accounts for both the uncertainty in knowing the value of the true population parameter and the variance of the data. Thus, the prediction interval is much wider than the confidence interval that just tells you the likely location of the true point. 